{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "willing-profile",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Sequence, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from pytorch_lightning import LightningModule, LightningDataModule, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-copyright",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-given",
   "metadata": {},
   "source": [
    "### Model Defined by Pytorch Module\n",
    "\n",
    "A basic way to define a neural network in Pytorch framework is implemented by Pytorch Module class.\n",
    "Module class has two methods to implement.\n",
    "\n",
    "- __init__\n",
    "  - Inherit and initialize Module class\n",
    "  - Build-up your model architecture in this method\n",
    "- forward\n",
    "  - Define the forward propagation of your model\n",
    "  - Note that the backward propagation is implemented by auto gradient if your forward propagation is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-wisdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, padding=1, output_padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.PReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=1, padding=1, output_padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mu      = nn.Linear(784, 32)\n",
    "        self.logvar  = nn.Linear(784, 32)\n",
    "        self.bridge  = nn.Linear(32, 784)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std).mul(std)\n",
    "            z   = mu + eps\n",
    "\n",
    "        else:\n",
    "            z   = mu\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        x      = self.encoder(x)\n",
    "        x      = self.flatten(x)\n",
    "        mu     = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        z      = self.reparameterize(mu, logvar)\n",
    "        x_hat  = self.decoder(self.bridge(z).view(-1, 16, 7, 7))\n",
    "\n",
    "        return {\"mu\": mu, \"logvar\": logvar, \"x_hat\": x_hat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model\n",
    "model = Model()\n",
    "\n",
    "# Test this model\n",
    "image  = torch.rand(16, 1, 28, 28) # B x C x H x W\n",
    "output = model(image)\n",
    "\n",
    "for key in output:\n",
    "    print(\"shape of {}:\".format(key), output[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-version",
   "metadata": {},
   "source": [
    "### Typical Training Process\n",
    "\n",
    "- Preparation\n",
    "  - Initialize optimizer\n",
    "  - Initialize dataset and data laoder\n",
    "  - Initialize learning rate scheduler (optional)\n",
    "- Training (At Each Epoch)\n",
    "  - Clear gradients (optimizer.zero_grad()) and get batch data from data loader\n",
    "  - Forward propagation and compute loss\n",
    "  - Backward propagation (loss.backward()) and update parameters (optimizer.step())\n",
    "  - Evaluate on validation set (optional but highly recommended)\n",
    "  - Decay learning rate (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-montana",
   "metadata": {},
   "source": [
    "### Pytroch Lightning\n",
    "\n",
    "In the training loop, there are many details that seems redundant in your code.\n",
    "For instance, optimizer.zero_grad(), loss.backward(), optimizer.step(), with torch.no_grad():, etc.\n",
    "Pytorch Lightning do these routines for you.\n",
    "\n",
    "In a LightningModule, you only implement the following methods and Pytorch Lightning Trainer will do others for you.\n",
    "- __init__: same in Pytorch Module\n",
    "- forward: same in Pytorch Module\n",
    "- training_step: define how you compute your loss\n",
    "- configure_optimizers: define your optimizer(s)\n",
    "- validation_step: define how you evaluate your model (optional but highly recommended)\n",
    "- validation_epoch: define how you summarize your evaluations (optional but highly recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(LightningModule):\n",
    "    def __init__(self, lr=1e-4, weight_decay=1e-4):\n",
    "        # 必備的 method，同 Pytorch Module\n",
    "        super(LitModel, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=8, padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, padding=1, output_padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.PReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=1, padding=1, output_padding=1, kernel_size=3, stride=2),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.mu      = nn.Linear(784, 32)\n",
    "        self.logvar  = nn.Linear(784, 32)\n",
    "        self.bridge  = nn.Linear(32, 784)\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        # 這個 method 單純是為了方便而定義的\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std).mul(std)\n",
    "            z   = mu + eps\n",
    "\n",
    "        else:\n",
    "            z   = mu\n",
    "\n",
    "        return z\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> dict:\n",
    "        # 必備的 method，同 Pytorch Module\n",
    "        x      = self.encoder(x)\n",
    "        x      = self.flatten(x)\n",
    "        mu     = self.mu(x)\n",
    "        logvar = self.logvar(x)\n",
    "        z      = self.reparameterize(mu, logvar)\n",
    "        x_hat  = self.decoder(self.bridge(z).view(-1, 16, 7, 7))\n",
    "\n",
    "        return {\"mu\": mu, \"logvar\": logvar, \"x_hat\": x_hat}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # 必備的 method，負責吐 optimizer 出來\n",
    "        return Adam(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "\n",
    "    def compute_vae_loss(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mu: torch.Tensor,\n",
    "        logvar: torch.Tensor,\n",
    "        x_hat: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        # 這是 method 單純是為了方便而定義的\n",
    "        bce = F.binary_cross_entropy(\n",
    "            torch.sigmoid(x_hat),\n",
    "            x,\n",
    "            reduction=\"none\"\n",
    "        ).sum(dim=(1, 2, 3)).mean()\n",
    "        kld = -0.5 * (1 + logvar - mu.pow(2) - logvar.exp()).sum(dim=1).mean()\n",
    "\n",
    "        return {\"loss\": bce + kld,\n",
    "                \"rec_loss\": bce,\n",
    "                \"kld_loss\": kld}\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        # 必備的 method，input 固定是吃兩個參數\n",
    "        # 吐你要拿去做 backward propagation 的 loss\n",
    "        X, y       = batch\n",
    "        output     = self(X)\n",
    "        losses     = self.compute_vae_loss(\n",
    "            x=X,\n",
    "            mu=output[\"mu\"],\n",
    "            logvar=output[\"logvar\"],\n",
    "            x_hat=output[\"x_hat\"]\n",
    "        )\n",
    "        total_loss = losses[\"loss\"]\n",
    "\n",
    "        # 利用 self.log 這個 method 把指標記錄到 tensorboard\n",
    "        # 若在 self.log 當中使用參數 prog_bar=True 可以幫你把你要的指標寫進 progress bar\n",
    "        # loss 原本就會被寫進 progress bar (by default)\n",
    "        self.log(\"train_loss\", total_loss)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int) -> Any:\n",
    "        # 可選用的 method，input 固定是吃兩個參數\n",
    "        # 吐你要算的指標\n",
    "        X, y   = batch\n",
    "        output = self(X)\n",
    "        losses = self.compute_vae_loss(\n",
    "            x=X,\n",
    "            mu=output[\"mu\"],\n",
    "            logvar=output[\"logvar\"],\n",
    "            x_hat=output[\"x_hat\"]\n",
    "        )\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def validation_epoch_end(self, outputs: Sequence) -> Any:\n",
    "        # 可選用的 method，input 固定是吃兩個參數\n",
    "        # 基本可以不用吐東西\n",
    "        total_loss = torch.mean(torch.stack([o[\"loss\"] for o in outputs]))\n",
    "        kld_loss   = torch.mean(torch.stack([o[\"kld_loss\"] for o in outputs]))\n",
    "        rec_loss   = torch.mean(torch.stack([o[\"rec_loss\"] for o in outputs]))\n",
    "\n",
    "        # 利用 self.log 這個 method 把指標記錄到 tensorboard\n",
    "        # 若在 self.log 當中使用參數 prog_bar=True 可以幫你把你要的指標寫進 progress bar\n",
    "        # validation 的階段沒有任何 default metric，所以要顯示在 progress bar 的都要自己指定\n",
    "        self.log(\"val_loss\", total_loss, prog_bar=True)\n",
    "        self.log(\"val_kld\", kld_loss, prog_bar=True)\n",
    "        self.log(\"val_rec\", rec_loss, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a lightning model\n",
    "lit_model = LitModel()\n",
    "\n",
    "# Test this model\n",
    "image  = torch.rand(16, 1, 28, 28) # B x C x H x W\n",
    "output = lit_model(image)\n",
    "\n",
    "for key in output:\n",
    "    print(\"shape of {}:\".format(key), output[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-cement",
   "metadata": {},
   "source": [
    "### Typical Pytorch Data Loader\n",
    "\n",
    "A typical Pytorch data loader needs a dataset as its parameter.\n",
    "Recall that a dataset is an iterable object in Python.\n",
    "We usually implenment 3 methods for a Pytorch dataset.\n",
    "\n",
    "- __init__\n",
    "- __getitem__\n",
    "- __len__\n",
    "\n",
    "By indicating the batch size, data loader automatically generate batch data parallelly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-newark",
   "metadata": {},
   "source": [
    "### Pytorch Lightning DataModule\n",
    "A LightningDataModule can be viewed as an end-to-end wrapping of Data Loader.\n",
    "Users can simply call a method defined in a LightingDataModule object and obtain a data loader.\n",
    "More precisely, we usually implement the following methods for a LightningDataModule.\n",
    "\n",
    "- __init__: inherit LightningDataModule and save hyperparameters\n",
    "- prepare_data: (optional but highly recommended)\n",
    "- train_dataloader: return a training data loader (optional but highly recommended)\n",
    "- val_dataloader: return a validation data laoder (optional but highly recommended)\n",
    "- test_dataloader: return a test data loader (optional but highly recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, batch_size: int = 256, data_dir: str = \"data\"):\n",
    "        super(DataModule, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.data_dir   = data_dir\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.val_data   = MNIST(self.data_dir, train=False, download=True, transform=ToTensor())\n",
    "        self.train_data = MNIST(self.data_dir, train=True, download=True, transform=ToTensor())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, batch_size=self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-kernel",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model  = LitModel()\n",
    "datamodule = DataModule()\n",
    "trainer    = Trainer()\n",
    "trainer.fit(lit_model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
